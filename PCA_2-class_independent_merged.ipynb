{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import  division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import warnings\n",
    "from mne.io import RawArray\n",
    "from mne.epochs import concatenate_epochs\n",
    "from mne import create_info, find_events, Epochs\n",
    "from mne.channels import read_custom_montage\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from glob import glob\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.signal import welch\n",
    "from mne import pick_types\n",
    "import os.path\n",
    "import scipy.io as sio\n",
    "from cnn_class import cnn\n",
    "import time\n",
    "from sklearn.metrics import classification_report, roc_auc_score, auc, roc_curve, f1_score\n",
    "from RnnAttention.attention import attention\n",
    "from scipy import interp\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings('ignore')\n",
    "##########################\n",
    "# create MNE readable file \n",
    "##########################\n",
    "def creat_mne_raw_object(fname):\n",
    "    \"\"\"Create a mne raw instance from csv file\"\"\"\n",
    "    # Read EEG file\n",
    "    data = pd.read_csv(fname)\n",
    "    \n",
    "    # get chanel names\n",
    "    ch_names = list(data.columns[1:])\n",
    "    \n",
    "    # read EEG standard montage from mne\n",
    "    montage = 'standard_1005'\n",
    "\n",
    "    # events file\n",
    "    ev_fname = fname.replace('_data','_events')\n",
    "    # read event file\n",
    "    events = pd.read_csv(ev_fname)\n",
    "    events_names = events.columns[1:]\n",
    "    events_data = np.array(events[events_names]).T\n",
    "    \n",
    "    # concatenate event file and data\n",
    "    data = np.concatenate((1e-6*np.array(data[ch_names]).T,events_data))        \n",
    "    \n",
    "    # define channel type, the first is EEG, the last 6 are stimulations\n",
    "    ch_type = ['eeg']*len(ch_names) + ['stim']*6\n",
    "    \n",
    "    # create and populate MNE info structure\n",
    "    ch_names.extend(events_names)\n",
    "    info = create_info(ch_names,sfreq=500.0,ch_types=ch_type)\n",
    "    # info.set_montage(montage)\n",
    "    #info['filename'] = fname\n",
    "    \n",
    "    # create raw object \n",
    "    raw = RawArray(data,info,verbose=False)\n",
    "    return raw\n",
    "\n",
    "\n",
    "####################################################################\n",
    "# Data preprocessing, band_pass, event related 1.5s + 1.5s featuring\n",
    "####################################################################\n",
    "def data_gen(subjects):    \n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    n = 0\n",
    "    for i in range(len(subjects)):\n",
    "        n+=1\n",
    "        subject = subjects[i]\n",
    "        epochs_tot = []\n",
    "        epochs_rest_tot = []\n",
    "        fnames =  glob('../EEG/train/subj%d_series*_data.csv' % (subject))\n",
    "\n",
    "        y = []\n",
    "        for i,fname in enumerate(fnames):\n",
    "            # read data \n",
    "            raw = creat_mne_raw_object(fname)\n",
    "            #raw.plot(block=True)\n",
    "            # pick eeg signal\n",
    "            picks = pick_types(raw.info,eeg=True)\n",
    "            raw.filter(7,35, picks=picks, method='iir', n_jobs=-1, verbose=False)        \n",
    "            # Filter data for alpha frequency and beta band\n",
    "            ##########\n",
    "            # Events #\n",
    "            ##########\n",
    "            ###############id = 'Replace'#################        \n",
    "            # get event posision corresponding to Replace\n",
    "            events = find_events(raw,stim_channel='Replace', verbose=False)\n",
    "            # epochs signal for 1.5 second before the movement\n",
    "            epochs = Epochs(raw, events, {'after' : 1}, 0.5, 2, proj=False, picks=picks, baseline=None, \n",
    "                            preload=True, verbose=False)\n",
    "            epochs_tot.append(epochs)        \n",
    "            # epochs signal for 1.5 second after the movement, this correspond to the rest period.\n",
    "            epochs_rest = Epochs(raw, events, {'during' : 1}, -2, -0.5, proj=False, picks=picks, baseline=None, \n",
    "                                 preload=True, verbose=False)\n",
    "            # Workaround to be able to concatenate epochs\n",
    "            epochs_rest.shift_time(2.5, relative = True)\n",
    "            epochs_rest_tot.append(epochs_rest)\n",
    "            # adding \"Replace\" lable \"5\" \n",
    "            y.extend([5]*len(epochs_rest))\n",
    "\n",
    "            #################### id = 'HandStart'####################\n",
    "            # get event posision corresponding to HandStart\n",
    "            events = find_events(raw,stim_channel='HandStart', verbose=False)\n",
    "            # epochs signal for 1.5 second before the movement\n",
    "            epochs = Epochs(raw, events, {'during' : 1}, 0.5, 2, proj=False, picks=picks, baseline=None, \n",
    "                            preload=True, verbose=False)       \n",
    "            epochs_tot.append(epochs)        \n",
    "            # epochs signal for 1.5 second after the movement, this correspond to the rest period.\n",
    "            epochs_rest = Epochs(raw, events, {'before' : 1}, -2, -0.5, proj=False, picks=picks, baseline=None, \n",
    "                                 preload=True, verbose=False)\n",
    "\n",
    "            # Workaround to be able to concatenate epochs\n",
    "            epochs_rest.shift_time(2.5, relative = True)\n",
    "            epochs_rest_tot.append(epochs_rest)\n",
    "            # adding lable \"1\" of \"HandStart\"\n",
    "            y.extend([1]*len(epochs_rest))\n",
    "            '''\n",
    "            #################### id = 'FirstDigitTouch'####################\n",
    "            # get event posision corresponding to FirstDigitTouch\n",
    "            events = find_events(raw,stim_channel='FirstDigitTouch', verbose=False)\n",
    "            epochs = Epochs(raw, events, {'during' : 1}, 0.5, 2, proj=False, picks=picks, baseline=None, \n",
    "                            preload=True, verbose=False)       \n",
    "            epochs_tot.append(epochs)        \n",
    "            epochs_rest = Epochs(raw, events, {'before' : 1}, -2, -0.5, proj=False, picks=picks, baseline=None, \n",
    "                                 preload=True, verbose=False)\n",
    "            epochs_rest.shift_time(2.5, relative = True)\n",
    "            epochs_rest_tot.append(epochs_rest)\n",
    "            # adding lable \"2\" of \"FirstDigitTouch\"\n",
    "            y.extend([2]*len(epochs_rest))\n",
    "\n",
    "            #################### id = 'BothStartLoadPhase'####################\n",
    "            # get event posision corresponding to BothStartLoadPh - truncated to 15 characters\n",
    "            events = find_events(raw,stim_channel='BothStartLoadPh', verbose=False)\n",
    "            epochs = Epochs(raw, events, {'during' : 1}, 0.5, 2, proj=False, picks=picks, baseline=None, \n",
    "                            preload=True, verbose=False)       \n",
    "            epochs_tot.append(epochs)        \n",
    "            epochs_rest = Epochs(raw, events, {'before' : 1}, -2, -0.5, proj=False, picks=picks, baseline=None, \n",
    "                                 preload=True, verbose=False)        \n",
    "            epochs_rest.shift_time(2.5, relative = True)\n",
    "            epochs_rest_tot.append(epochs_rest)\n",
    "            # adding lable \"3\" of \"FirstDigitTouch\"\n",
    "            y.extend([3]*len(epochs_rest))\n",
    "            \n",
    "            \n",
    "            #################### id = 'Liftoff'####################\n",
    "            # get event posision corresponding to Liftoff \n",
    "            events = find_events(raw,stim_channel='LiftOff', verbose=False)\n",
    "            epochs = Epochs(raw, events, {'during' : 1}, 0.5, 2, proj=False, picks=picks, baseline=None, \n",
    "                            preload=True, verbose=False)       \n",
    "            epochs_tot.append(epochs)        \n",
    "            epochs_rest = Epochs(raw, events, {'before' : 1}, -2, -0.5, proj=False, picks=picks, baseline=None, \n",
    "                                 preload=True, verbose=False)        \n",
    "            epochs_rest.shift_time(2.5, relative = True)\n",
    "            epochs_rest_tot.append(epochs_rest)\n",
    "            # adding lable \"4\" of \"FirstDigitTouch\"\n",
    "            y.extend([4]*len(epochs_rest))\n",
    "\n",
    "            #################### id = 'BothReleased'####################\n",
    "            # get event posision corresponding to BothReleased \n",
    "            events = find_events(raw,stim_channel='BothReleased', verbose=False)\n",
    "            epochs = Epochs(raw, events, {'during' : 1}, 0.5, 2, proj=False, picks=picks, baseline=None, \n",
    "                            preload=True, verbose=False)       \n",
    "            epochs_tot.append(epochs)        \n",
    "            epochs_rest = Epochs(raw, events, {'before' : 1}, -2, -0.5, proj=False, picks=picks, baseline=None, \n",
    "                                 preload=True, verbose=False)        \n",
    "            epochs_rest.shift_time(2.5, relative = True)\n",
    "            epochs_rest_tot.append(epochs_rest)\n",
    "            # adding lable \"6\" of \"BothReleased\"\n",
    "            y.extend([6]*len(epochs_rest))\n",
    "            '''\n",
    "\n",
    "        epochs_during = concatenate_epochs(epochs_tot)\n",
    "        epochs_rests = concatenate_epochs(epochs_rest_tot)\n",
    "\n",
    "        #get data \n",
    "        X_during = epochs_during.get_data()\n",
    "        X_rests = epochs_rests.get_data()\n",
    "        \n",
    "        X = np.concatenate((X_during,X_rests),axis=1)\n",
    "        \n",
    "        pca = UnsupervisedSpatialFilter(PCA(22), average=False)\n",
    "        X = pca.fit_transform(X)\n",
    "        print (\"'X' after PCA shape: \",X.shape)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        #y = np.array(y)\n",
    "        print (\"subject\",subject,X.shape)\n",
    "\n",
    "        if n == 1:\n",
    "            X_train = X\n",
    "            y_train = y\n",
    "        else:\n",
    "            X_train = np.append(X_train,X,axis =0)\n",
    "            y_train = np.append(y_train,y,axis =0)\n",
    "\n",
    "        print (\"data stack shape:\",X_train.shape,y_train.shape)\n",
    "\n",
    "    # generate feature dataset for next process \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    print ('return data shape: ',X_train.shape,y_train.shape)\n",
    "    return (X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 2 (520, 22, 751)\n",
      "data stack shape: (520, 22, 751) (520,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 3 (520, 22, 751)\n",
      "data stack shape: (1040, 22, 751) (1040,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 4 (520, 22, 751)\n",
      "data stack shape: (1560, 22, 751) (1560,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 5 (520, 22, 751)\n",
      "data stack shape: (2080, 22, 751) (2080,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 6 (520, 22, 751)\n",
      "data stack shape: (2600, 22, 751) (2600,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 7 (520, 22, 751)\n",
      "data stack shape: (3120, 22, 751) (3120,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 8 (520, 22, 751)\n",
      "data stack shape: (3640, 22, 751) (3640,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 9 (520, 22, 751)\n",
      "data stack shape: (4160, 22, 751) (4160,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 10 (520, 22, 751)\n",
      "data stack shape: (4680, 22, 751) (4680,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 11 (520, 22, 751)\n",
      "data stack shape: (5200, 22, 751) (5200,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 12 (520, 22, 751)\n",
      "data stack shape: (5720, 22, 751) (5720,)\n",
      "return data shape:  (5720, 22, 751) (5720,)\n",
      "'X' after PCA shape:  (520, 22, 751)\n",
      "subject 1 (520, 22, 751)\n",
      "data stack shape: (520, 22, 751) (520,)\n",
      "return data shape:  (520, 22, 751) (520,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 1, 5, 1, 5, 5, 1, 1, 1, 5, 1, 5, 5, 1, 5, 1, 1, 1,\n",
       "       5, 1, 1, 5, 1, 1, 5, 5, 5, 1, 5, 1, 5, 5, 1, 5, 1, 5, 5, 1, 1, 5,\n",
       "       5, 1, 1, 5, 1, 5, 5, 1, 5, 5, 1, 1, 5, 1, 1, 5, 1, 1, 5, 1, 1, 1,\n",
       "       1, 5, 1, 5, 5, 5, 1, 5, 1, 1, 5, 5, 1, 5, 5, 1, 5, 5, 5, 1, 5, 1,\n",
       "       1, 1, 5, 1, 1, 5, 5, 1, 5, 5, 1, 5, 1, 5, 5, 5, 5, 5, 1, 5, 1, 1,\n",
       "       1, 1, 5, 5, 1, 1, 1, 1, 5, 5, 5, 1, 1, 1, 5, 5, 5, 1, 5, 1, 5, 1,\n",
       "       1, 1, 1, 5, 5, 5, 1, 1, 1, 5, 5, 5, 5, 1, 1, 1, 1, 5, 1, 5, 5, 5,\n",
       "       5, 1, 5, 5, 1, 5, 1, 5, 1, 5, 1, 1, 5, 1, 1, 5, 1, 5, 1, 5, 1, 1,\n",
       "       1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 1, 1, 1, 1, 1, 1,\n",
       "       5, 1, 5, 1, 5, 1, 5, 5, 5, 1, 1, 1, 1, 5, 1, 5, 1, 1, 1, 1, 5, 5,\n",
       "       5, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 1, 1, 1, 5, 5, 1, 1, 1, 5, 5,\n",
       "       1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 5, 5, 5, 1, 5, 5, 5, 5, 1, 5, 5, 5,\n",
       "       5, 1, 1, 1, 1, 1, 5, 5, 1, 1, 1, 5, 1, 1, 1, 1, 5, 5, 5, 1, 5, 1,\n",
       "       1, 5, 1, 5, 5, 1, 1, 5, 1, 5, 1, 1, 1, 1, 5, 5, 1, 1, 5, 5, 1, 1,\n",
       "       5, 5, 5, 5, 1, 5, 5, 5, 1, 5, 5, 1, 1, 5, 1, 1, 5, 5, 5, 1, 1, 5,\n",
       "       1, 1, 1, 5, 5, 1, 5, 1, 1, 1, 5, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 1,\n",
       "       1, 1, 5, 1, 5, 5, 1, 1, 1, 5, 1, 1, 5, 1, 5, 5, 1, 5, 1, 1, 5, 1,\n",
       "       5, 1, 5, 5, 1, 5, 5, 5, 5, 1, 1, 1, 5, 5, 1, 1, 1, 5, 1, 5, 1, 5,\n",
       "       5, 1, 5, 5, 5, 1, 1, 5, 1, 1, 1, 1, 5, 5, 5, 5, 1, 1, 5, 1, 5, 5,\n",
       "       5, 5, 5, 1, 1, 5, 5, 5, 5, 1, 5, 5, 1, 5, 5, 5, 1, 1, 5, 1, 1, 1,\n",
       "       5, 1, 1, 1, 5, 1, 5, 1, 1, 5, 5, 5, 5, 5, 1, 1, 5, 5, 1, 5, 1, 5,\n",
       "       5, 5, 5, 5, 5, 1, 5, 1, 1, 5, 1, 5, 1, 1, 1, 5, 1, 1, 5, 5, 1, 5,\n",
       "       5, 5, 5, 5, 1, 1, 5, 5, 5, 5, 5, 5, 1, 1, 1, 1, 5, 1, 1, 5, 1, 1,\n",
       "       5, 1, 1, 5, 1, 1, 5, 5, 5, 1, 5, 5, 5, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_win_x shape:  (5720, 14, 100, 22)\n",
      "test_win_x shape:  (520, 14, 100, 22)\n",
      "train_win_x shape:  (5720, 14, 22, 100)\n",
      "test_win_x shape:  (520, 14, 22, 100)\n",
      "WARNING:tensorflow:From /home/yaoxiaojian/Desktop/kaggle/EEG/cnn_class.py:17: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yaoxiaojian/Desktop/kaggle/EEG/cnn_class.py:71: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7ff36c64cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7ff36c64cbd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7ff36c64cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7ff36c64cbd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "conv 1 shape:  [None, 1, 56, 40]\n",
      "WARNING:tensorflow:From /home/yaoxiaojian/Desktop/kaggle/EEG/cnn_class.py:84: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "pool 1 shape:  [None, 1, 1, 40]\n",
      "WARNING:tensorflow:From <ipython-input-2-fccfd9718326>:255: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-fccfd9718326>:266: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-2-fccfd9718326>:269: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-2-fccfd9718326>:274: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7ff36d222b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7ff36d222b10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7ff36d222b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7ff36d222b10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From /home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/yaoxiaojian/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c64cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c64cbd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c64cbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c64cbd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c91bad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c91bad0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c91bad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7ff36c91bad0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /home/yaoxiaojian/Desktop/kaggle/EEG/RnnAttention/attention.py:57: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-fccfd9718326>:293: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "(Sat Dec  5 15:09:01 2020) Epoch:  1  Training Cost:  0.6713527711954984 Training Accuracy:  0.5973776272152895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sat Dec  5 15:09:06 2020) Epoch:  1 Test Cost:  0.6968417924184066 Test Accuracy:  0.5122377698446487 Test f1:  0.36224300176906954 Test AUC:  0.5216921345290658 \n",
      "\n",
      "(Sat Dec  5 15:09:23 2020) Epoch:  2  Training Cost:  0.5952839398092323 Training Accuracy:  0.7122377637591395\n",
      "(Sat Dec  5 15:09:28 2020) Epoch:  2 Test Cost:  0.6846255915356683 Test Accuracy:  0.5568181868981231 Test f1:  0.5306345581109939 Test AUC:  0.5751575432897384 \n",
      "\n",
      "(Sat Dec  5 15:09:45 2020) Epoch:  3  Training Cost:  0.562811551967284 Training Accuracy:  0.719055943868377\n",
      "(Sat Dec  5 15:09:49 2020) Epoch:  3 Test Cost:  0.7063668873551843 Test Accuracy:  0.5555944101220661 Test f1:  0.4815233519596739 Test AUC:  0.5464440072074722 \n",
      "\n",
      "(Sat Dec  5 15:10:07 2020) Epoch:  4  Training Cost:  0.539355765689503 Training Accuracy:  0.7246503511092046\n",
      "(Sat Dec  5 15:10:11 2020) Epoch:  4 Test Cost:  0.7145532596778202 Test Accuracy:  0.5452797249923428 Test f1:  0.468089773177384 Test AUC:  0.5211197095621304 \n",
      "\n",
      "(Sat Dec  5 15:10:28 2020) Epoch:  5  Training Cost:  0.4946694850817427 Training Accuracy:  0.7736013977677672\n",
      "(Sat Dec  5 15:10:33 2020) Epoch:  5 Test Cost:  0.6955444884675366 Test Accuracy:  0.554020985075227 Test f1:  0.5171316806711204 Test AUC:  0.5764011742319327 \n",
      "\n",
      "(Sat Dec  5 15:10:50 2020) Epoch:  6  Training Cost:  0.46782412478973817 Training Accuracy:  0.8064685286237643\n",
      "(Sat Dec  5 15:10:54 2020) Epoch:  6 Test Cost:  0.6785116075844198 Test Accuracy:  0.5823426631546312 Test f1:  0.5823389681635349 Test AUC:  0.6002614193772016 \n",
      "\n",
      "(Sat Dec  5 15:11:12 2020) Epoch:  7  Training Cost:  0.44490978054024954 Training Accuracy:  0.8129370586126001\n",
      "(Sat Dec  5 15:11:16 2020) Epoch:  7 Test Cost:  0.6867499199363735 Test Accuracy:  0.5503496603610424 Test f1:  0.5446129966982226 Test AUC:  0.596285691754527 \n",
      "\n",
      "(Sat Dec  5 15:11:33 2020) Epoch:  8  Training Cost:  0.4388118839295177 Training Accuracy:  0.8178321638932595\n",
      "(Sat Dec  5 15:11:38 2020) Epoch:  8 Test Cost:  0.7035136341631829 Test Accuracy:  0.530594413331547 Test f1:  0.5231858050679926 Test AUC:  0.5602139208492583 \n",
      "\n",
      "(Sat Dec  5 15:11:55 2020) Epoch:  9  Training Cost:  0.45731382908208384 Training Accuracy:  0.7694055935094407\n",
      "(Sat Dec  5 15:11:59 2020) Epoch:  9 Test Cost:  0.764009646394036 Test Accuracy:  0.5220279787126538 Test f1:  0.42888362190173945 Test AUC:  0.5273770270436153 \n",
      "\n",
      "(Sat Dec  5 15:12:17 2020) Epoch:  10  Training Cost:  0.410674097342091 Training Accuracy:  0.8251748207982603\n",
      "(Sat Dec  5 15:12:21 2020) Epoch:  10 Test Cost:  0.7344291641578807 Test Accuracy:  0.5332167892736364 Test f1:  0.4963276944971087 Test AUC:  0.5702283677404602 \n",
      "\n",
      "(Sat Dec  5 15:12:38 2020) Epoch:  11  Training Cost:  0.3960021131678478 Training Accuracy:  0.8298951006540052\n",
      "(Sat Dec  5 15:12:43 2020) Epoch:  11 Test Cost:  0.7164836946900908 Test Accuracy:  0.5444056001211797 Test f1:  0.5072860991290263 Test AUC:  0.574600928084 \n",
      "\n",
      "(Sat Dec  5 15:13:01 2020) Epoch:  12  Training Cost:  0.39886519138317006 Training Accuracy:  0.8365384570055908\n",
      "(Sat Dec  5 15:13:05 2020) Epoch:  12 Test Cost:  0.7035615614035746 Test Accuracy:  0.5096153905940848 Test f1:  0.49840835642186954 Test AUC:  0.5608522821030225 \n",
      "\n",
      "(Sat Dec  5 15:13:23 2020) Epoch:  13  Training Cost:  0.41388498832816845 Training Accuracy:  0.8099650315992482\n",
      "(Sat Dec  5 15:13:27 2020) Epoch:  13 Test Cost:  0.7473904811418973 Test Accuracy:  0.5297202844094563 Test f1:  0.4482485797424739 Test AUC:  0.5805660148104616 \n",
      "\n",
      "(Sat Dec  5 15:13:45 2020) Epoch:  14  Training Cost:  0.39508414594130914 Training Accuracy:  0.831118876388023\n",
      "(Sat Dec  5 15:13:49 2020) Epoch:  14 Test Cost:  0.7181978832174848 Test Accuracy:  0.5451049015082262 Test f1:  0.4865260809301776 Test AUC:  0.6054841489265301 \n",
      "\n",
      "(Sat Dec  5 15:14:06 2020) Epoch:  15  Training Cost:  0.3777462319321149 Training Accuracy:  0.8590909029011959\n",
      "(Sat Dec  5 15:14:11 2020) Epoch:  15 Test Cost:  0.670548755962115 Test Accuracy:  0.5659090968278738 Test f1:  0.5611606898016723 Test AUC:  0.6353962900408187 \n",
      "\n",
      "(Sat Dec  5 15:14:28 2020) Epoch:  16  Training Cost:  0.36895002265805965 Training Accuracy:  0.8494755182337094\n",
      "(Sat Dec  5 15:14:32 2020) Epoch:  16 Test Cost:  0.7603312193096935 Test Accuracy:  0.5374125934668355 Test f1:  0.484020618556701 Test AUC:  0.5919566134029675 \n",
      "\n",
      "(Sat Dec  5 15:14:50 2020) Epoch:  17  Training Cost:  0.3850259334369973 Training Accuracy:  0.824650345357148\n",
      "(Sat Dec  5 15:14:54 2020) Epoch:  17 Test Cost:  0.7662820335138928 Test Accuracy:  0.5199300762578533 Test f1:  0.44742302046183224 Test AUC:  0.5936301523805712 \n",
      "\n",
      "(Sat Dec  5 15:15:12 2020) Epoch:  18  Training Cost:  0.3590765636596646 Training Accuracy:  0.8442307636141777\n",
      "(Sat Dec  5 15:15:16 2020) Epoch:  18 Test Cost:  0.7749083921625898 Test Accuracy:  0.5395104939026849 Test f1:  0.4762446666677791 Test AUC:  0.6133826125402209 \n",
      "\n",
      "(Sat Dec  5 15:15:33 2020) Epoch:  19  Training Cost:  0.37052409145069287 Training Accuracy:  0.834440554251204\n",
      "(Sat Dec  5 15:15:38 2020) Epoch:  19 Test Cost:  0.771289517985774 Test Accuracy:  0.5479021030966635 Test f1:  0.48112537751209355 Test AUC:  0.6193943098143311 \n",
      "\n",
      "(Sat Dec  5 15:15:55 2020) Epoch:  20  Training Cost:  0.3863389151548589 Training Accuracy:  0.835664330975159\n",
      "(Sat Dec  5 15:15:59 2020) Epoch:  20 Test Cost:  0.7306442750500632 Test Accuracy:  0.5611888133249916 Test f1:  0.49192182166969783 Test AUC:  0.6605834363576819 \n",
      "\n",
      "(Sat Dec  5 15:16:16 2020) Epoch:  21  Training Cost:  0.36420838709373576 Training Accuracy:  0.878496497244268\n",
      "(Sat Dec  5 15:16:21 2020) Epoch:  21 Test Cost:  0.6816126304072934 Test Accuracy:  0.5669580516958987 Test f1:  0.5558395736593585 Test AUC:  0.6219993506569311 \n",
      "\n",
      "(Sat Dec  5 15:16:38 2020) Epoch:  22  Training Cost:  0.3464407616822453 Training Accuracy:  0.8818181752637549\n",
      "(Sat Dec  5 15:16:43 2020) Epoch:  22 Test Cost:  0.7142635587003682 Test Accuracy:  0.578671333699168 Test f1:  0.5568243558213178 Test AUC:  0.6140372488094501 \n",
      "\n",
      "(Sat Dec  5 15:17:00 2020) Epoch:  23  Training Cost:  0.3296670516679337 Training Accuracy:  0.8767482452876084\n",
      "(Sat Dec  5 15:17:04 2020) Epoch:  23 Test Cost:  0.7197307768714177 Test Accuracy:  0.5592657412578176 Test f1:  0.5247741385952065 Test AUC:  0.6044160473304137 \n",
      "\n",
      "(Sat Dec  5 15:17:22 2020) Epoch:  24  Training Cost:  0.3531993068494163 Training Accuracy:  0.8732517419474108\n",
      "(Sat Dec  5 15:17:26 2020) Epoch:  24 Test Cost:  0.6933522865280405 Test Accuracy:  0.570454550060359 Test f1:  0.5313131388587424 Test AUC:  0.6424148270635561 \n",
      "\n",
      "(Sat Dec  5 15:17:43 2020) Epoch:  25  Training Cost:  0.3154942552083647 Training Accuracy:  0.8833916011181745\n",
      "(Sat Dec  5 15:17:48 2020) Epoch:  25 Test Cost:  0.7126296492931726 Test Accuracy:  0.5767482600949861 Test f1:  0.5526727619320067 Test AUC:  0.6335074012253542 \n",
      "\n",
      "(Sat Dec  5 15:18:05 2020) Epoch:  26  Training Cost:  0.3448429381592082 Training Accuracy:  0.8409090858552005\n",
      "(Sat Dec  5 15:18:09 2020) Epoch:  26 Test Cost:  0.9097013595950353 Test Accuracy:  0.5335664388860439 Test f1:  0.45049557428313985 Test AUC:  0.6049506748219933 \n",
      "\n",
      "(Sat Dec  5 15:18:27 2020) Epoch:  27  Training Cost:  0.32651982565859816 Training Accuracy:  0.8615384559843924\n",
      "(Sat Dec  5 15:18:31 2020) Epoch:  27 Test Cost:  0.8526043078595108 Test Accuracy:  0.5554195835769593 Test f1:  0.4843918942893427 Test AUC:  0.6024087110532841 \n",
      "\n",
      "(Sat Dec  5 15:18:48 2020) Epoch:  28  Training Cost:  0.37058294447256135 Training Accuracy:  0.8265734219571927\n",
      "(Sat Dec  5 15:18:53 2020) Epoch:  28 Test Cost:  0.829105117521086 Test Accuracy:  0.5470279756855297 Test f1:  0.4655826958142494 Test AUC:  0.6075434369011062 \n",
      "\n",
      "(Sat Dec  5 15:19:10 2020) Epoch:  29  Training Cost:  0.3341089055020284 Training Accuracy:  0.8727272656205651\n",
      "(Sat Dec  5 15:19:14 2020) Epoch:  29 Test Cost:  0.7610107221074037 Test Accuracy:  0.5393356724635704 Test f1:  0.49996640941531645 Test AUC:  0.5802202313365205 \n",
      "\n",
      "(Sat Dec  5 15:19:32 2020) Epoch:  30  Training Cost:  0.2998889141123403 Training Accuracy:  0.8749999929141331\n",
      "(Sat Dec  5 15:19:36 2020) Epoch:  30 Test Cost:  0.8010344177588716 Test Accuracy:  0.5569930124532926 Test f1:  0.5123890262569175 Test AUC:  0.5971209157821687 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Sat Dec  5 15:19:53 2020) Epoch:  31  Training Cost:  0.28015189387321054 Training Accuracy:  0.900874118600692\n",
      "(Sat Dec  5 15:19:58 2020) Epoch:  31 Test Cost:  0.756666843551439 Test Accuracy:  0.5664335706017234 Test f1:  0.5432759672287886 Test AUC:  0.6076755075366393 \n",
      "\n",
      "(Sat Dec  5 15:20:15 2020) Epoch:  32  Training Cost:  0.3033362544057044 Training Accuracy:  0.8819929995424264\n",
      "(Sat Dec  5 15:20:20 2020) Epoch:  32 Test Cost:  0.8071047446423477 Test Accuracy:  0.5687063002398798 Test f1:  0.532343586238323 Test AUC:  0.6093770493301108 \n",
      "\n",
      "(Sat Dec  5 15:20:37 2020) Epoch:  33  Training Cost:  0.37016461191737987 Training Accuracy:  0.8132867099313469\n",
      "(Sat Dec  5 15:20:41 2020) Epoch:  33 Test Cost:  0.9116590642533102 Test Accuracy:  0.5201048988041345 Test f1:  0.3988431020467797 Test AUC:  0.5982626828114395 \n",
      "\n",
      "(Sat Dec  5 15:20:59 2020) Epoch:  34  Training Cost:  0.3740440914342245 Training Accuracy:  0.8134615340016105\n",
      "(Sat Dec  5 15:21:03 2020) Epoch:  34 Test Cost:  0.8999605163723439 Test Accuracy:  0.5337412621487271 Test f1:  0.4246197543204606 Test AUC:  0.6036668853308826 \n",
      "\n",
      "(Sat Dec  5 15:21:20 2020) Epoch:  35  Training Cost:  0.3245484047940561 Training Accuracy:  0.8888111812668247\n",
      "(Sat Dec  5 15:21:25 2020) Epoch:  35 Test Cost:  0.7363501004093177 Test Accuracy:  0.5547202835966657 Test f1:  0.5092571343340232 Test AUC:  0.6267619953278863 \n",
      "\n",
      "(Sat Dec  5 15:21:42 2020) Epoch:  36  Training Cost:  0.3444302780879336 Training Accuracy:  0.8377622325833027\n",
      "(Sat Dec  5 15:21:46 2020) Epoch:  36 Test Cost:  0.8962165336613055 Test Accuracy:  0.5236014042805125 Test f1:  0.4273564326881128 Test AUC:  0.5741442394694052 \n",
      "\n",
      "(Sat Dec  5 15:22:04 2020) Epoch:  37  Training Cost:  0.34669541574076757 Training Accuracy:  0.8325174783284848\n",
      "(Sat Dec  5 15:22:08 2020) Epoch:  37 Test Cost:  0.9822044039106036 Test Accuracy:  0.525699304586107 Test f1:  0.42198056569276116 Test AUC:  0.5987290931112768 \n",
      "\n",
      "(Sat Dec  5 15:22:26 2020) Epoch:  38  Training Cost:  0.40195866932089513 Training Accuracy:  0.8040209755926699\n",
      "(Sat Dec  5 15:22:30 2020) Epoch:  38 Test Cost:  1.3182559198849684 Test Accuracy:  0.5061188849223244 Test f1:  0.3715334727168559 Test AUC:  0.565513633420876 \n",
      "\n",
      "(Sat Dec  5 15:22:48 2020) Epoch:  39  Training Cost:  0.3028244133647922 Training Accuracy:  0.8631118823077295\n",
      "(Sat Dec  5 15:22:52 2020) Epoch:  39 Test Cost:  1.0352653383791863 Test Accuracy:  0.5260489566342814 Test f1:  0.42911504718320187 Test AUC:  0.5965188558186996 \n",
      "\n",
      "(Sat Dec  5 15:23:10 2020) Epoch:  40  Training Cost:  0.3189961784161054 Training Accuracy:  0.8592657280134988\n",
      "(Sat Dec  5 15:23:14 2020) Epoch:  40 Test Cost:  1.0083527617104404 Test Accuracy:  0.5202797265475864 Test f1:  0.4190197045764532 Test AUC:  0.5710296732107372 \n",
      "\n",
      "(Sat Dec  5 15:23:31 2020) Epoch:  41  Training Cost:  0.3703219814004598 Training Accuracy:  0.8246503461907794\n",
      "(Sat Dec  5 15:23:36 2020) Epoch:  41 Test Cost:  1.0374014538068037 Test Accuracy:  0.5201049010445188 Test f1:  0.4064503866441924 Test AUC:  0.5957448571236235 \n",
      "\n",
      "(Sat Dec  5 15:23:53 2020) Epoch:  42  Training Cost:  0.36299917336318877 Training Accuracy:  0.8225524439053102\n",
      "(Sat Dec  5 15:23:57 2020) Epoch:  42 Test Cost:  1.0057109400004773 Test Accuracy:  0.5337412631907663 Test f1:  0.4292965688637411 Test AUC:  0.5910466301609484 \n",
      "\n",
      "(Sat Dec  5 15:24:15 2020) Epoch:  43  Training Cost:  0.33893186290312066 Training Accuracy:  0.8522727217499193\n",
      "(Sat Dec  5 15:24:19 2020) Epoch:  43 Test Cost:  0.9551424568558072 Test Accuracy:  0.5279720328696125 Test f1:  0.4236032455571096 Test AUC:  0.5838017951741836 \n",
      "\n",
      "(Sat Dec  5 15:24:36 2020) Epoch:  44  Training Cost:  0.4419409442185001 Training Accuracy:  0.7711538440593473\n",
      "(Sat Dec  5 15:24:41 2020) Epoch:  44 Test Cost:  1.314580198142912 Test Accuracy:  0.502447557720271 Test f1:  0.3511646745388835 Test AUC:  0.5787723666302003 \n",
      "\n",
      "(Sat Dec  5 15:24:58 2020) Epoch:  45  Training Cost:  0.2611697450462546 Training Accuracy:  0.8942307626659219\n",
      "(Sat Dec  5 15:25:03 2020) Epoch:  45 Test Cost:  0.9851245785301382 Test Accuracy:  0.5594405618893517 Test f1:  0.5005596459511736 Test AUC:  0.5854657645362711 \n",
      "\n",
      "(Sat Dec  5 15:25:20 2020) Epoch:  46  Training Cost:  0.3302668604244302 Training Accuracy:  0.8424825128037613\n",
      "(Sat Dec  5 15:25:24 2020) Epoch:  46 Test Cost:  1.0045638561665595 Test Accuracy:  0.5281468568356721 Test f1:  0.419022924771005 Test AUC:  0.5968554740152927 \n",
      "\n",
      "(Sat Dec  5 15:25:42 2020) Epoch:  47  Training Cost:  0.2626415474561753 Training Accuracy:  0.9022727200201341\n",
      "(Sat Dec  5 15:25:46 2020) Epoch:  47 Test Cost:  0.9915362258370106 Test Accuracy:  0.5414335711019023 Test f1:  0.4725582879029607 Test AUC:  0.5849429006112528 \n",
      "\n",
      "(Sat Dec  5 15:26:04 2020) Epoch:  48  Training Cost:  0.3082837652857904 Training Accuracy:  0.8730769171998217\n",
      "(Sat Dec  5 15:26:09 2020) Epoch:  48 Test Cost:  0.9891407250107586 Test Accuracy:  0.5354895148869161 Test f1:  0.4459942203183837 Test AUC:  0.578752335732384 \n",
      "\n",
      "(Sat Dec  5 15:26:27 2020) Epoch:  49  Training Cost:  0.31098898511927026 Training Accuracy:  0.873076916835108\n",
      "(Sat Dec  5 15:26:32 2020) Epoch:  49 Test Cost:  1.0418632973011557 Test Accuracy:  0.5314685366787277 Test f1:  0.44110484023858476 Test AUC:  0.5771143038456051 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fccfd9718326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_timestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdropout_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_phase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m             \u001b[0;31m# calculate train and test accuracy after each training epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(1,13):\n",
    "    train_subject = [k for k in range(1,13) if k != j]\n",
    "    test_subject = [j]\n",
    "    train_X,train_y = data_gen(train_subject)\n",
    "    test_X,test_y = data_gen(test_subject)\n",
    "    \n",
    "    idx = list(range(len(train_y)))\n",
    "    np.random.shuffle(idx)\n",
    "    train_X = train_X[idx]\n",
    "    train_y = train_y[idx]    \n",
    "    \n",
    "    idx = list(range(len(test_y)))\n",
    "    np.random.shuffle(idx)\n",
    "    test_X = test_X[idx]\n",
    "    test_y = test_y[idx] \n",
    "    \n",
    "    train_X=train_X*100000\n",
    "    test_X=test_X*100000\n",
    "    # sio.savemat('/home/yaoxiaojian/Desktop/kaggle/EEG/TACR_indenpendent_4-class/T-set_sub'+str(j)+'.mat', {\"train_x\": train_X, \"train_y\": train_y, \"test_x\": test_X, \"test_y\": test_y})\n",
    "    \n",
    "    '''\n",
    "    # train_X preprocess\n",
    "    X_inputs = np.transpose(train_X, [1, 0, 2])\n",
    "    X_inputs=X_inputs.reshape((X_inputs.shape[0],(X_inputs.shape[1]*X_inputs.shape[2])))\n",
    "    X_inputs = np.transpose(X_inputs, [1, 0])    \n",
    "    \n",
    "    print ('X_inputs',X_inputs.shape)\n",
    "    # test_X preprocess\n",
    "    X_inputs1 = np.transpose(test_X, [1, 0, 2])\n",
    "    X_inputs1 = X_inputs1.reshape((X_inputs1.shape[0],(X_inputs1.shape[1]*X_inputs1.shape[2])))\n",
    "    X_inputs1 = np.transpose(X_inputs1, [1, 0])\n",
    "    print ('X_inputs1',X_inputs1.shape)\n",
    "\n",
    "    # X_inputs and Y_targets are np.array, while X,Y are tf.tensor class\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    training_epochs = 5\n",
    "    batch_size = 751\n",
    "    display_step = 1\n",
    "    n_input = 64\n",
    "    X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "    n_hidden_1 = 88\n",
    "    n_hidden_2 = 44\n",
    "    n_hidden_3 = 22\n",
    "\n",
    "    weights = {\n",
    "        'encoder_h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1], )),\n",
    "        'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], )),\n",
    "        'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], )),\n",
    "        'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_2], )),\n",
    "        'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_1], )),\n",
    "        'decoder_h3': tf.Variable(tf.truncated_normal([n_hidden_1, n_input], )),\n",
    "    }\n",
    "    biases = {\n",
    "        'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "        'decoder_b1': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'decoder_b2': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'decoder_b3': tf.Variable(tf.random_normal([n_input])),\n",
    "    }\n",
    "\n",
    "\n",
    "    def encoder(x):\n",
    "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                       biases['encoder_b1']))\n",
    "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                       biases['encoder_b2']))\n",
    "        layer_3 = tf.add(tf.matmul(layer_2, weights['encoder_h3']),\n",
    "                         biases['encoder_b3'])\n",
    "\n",
    "        return layer_3\n",
    "\n",
    "\n",
    "    def decoder(x):\n",
    "        layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                       biases['decoder_b1']))\n",
    "        layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                       biases['decoder_b2']))\n",
    "        layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n",
    "                                       biases['decoder_b3']))\n",
    "        return layer_3\n",
    "\n",
    "    # iteration\n",
    "    encoder_op = encoder(X)\n",
    "    decoder_op = decoder(encoder_op)\n",
    "\n",
    "    y_pred = decoder_op\n",
    "    y_true = X\n",
    "\n",
    "    cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) # cost func. MSE\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    encoder_result = []\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        total_batch = int(X_inputs.shape[0] / batch_size)\n",
    "        for epoch in range(training_epochs):\n",
    "            for i in range(total_batch):\n",
    "                offset = (i * batch_size) \n",
    "                batch_xs = X_inputs[offset:(offset + batch_size), :] \n",
    "                #batch_ys = Y_targets[offset:(offset + batch_size), :]\n",
    "                print ('.',end = '')\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "            if epoch % display_step == 0:\n",
    "                print()\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(c))\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # build encoder_result array as feature vector\n",
    "        train_X=sess.run(encoder_op, feed_dict={X: X_inputs})      \n",
    "        train_X=train_X.reshape(int(train_X.shape[0]/751),751,22)\n",
    "        train_X=np.transpose(train_X,[0,2,1])\n",
    "\n",
    "        test_X=sess.run(encoder_op, feed_dict={X: X_inputs1})      \n",
    "        test_X=test_X.reshape(int(test_X.shape[0]/751),751,22)\n",
    "        test_X=np.transpose(test_X,[0,2,1])\n",
    "\n",
    "        print (train_X.shape, test_X.shape, train_y.shape,test_y.shape)\n",
    "        display (test_X)\n",
    "\n",
    "    sess.close()\n",
    "    '''\n",
    "    test_y\t= test_y.ravel()\n",
    "    train_y = train_y.ravel()\n",
    "    display (test_y)\n",
    "\n",
    "    train_y = np.asarray(pd.get_dummies(train_y), dtype = np.int8)\n",
    "    test_y = np.asarray(pd.get_dummies(test_y), dtype = np.int8)\n",
    "\n",
    "    #############\n",
    "    # Set window\n",
    "    #############\n",
    "\n",
    "    window_size = 100\n",
    "    step = 50\n",
    "    n_channel = 22\n",
    "\n",
    "    def windows(data, size, step):\n",
    "        start = 0\n",
    "        while ((start+size) < data.shape[0]):\n",
    "            yield int(start), int(start + size)\n",
    "            start += step\n",
    "\n",
    "    def segment_signal_without_transition(data, window_size, step):\n",
    "        segments = []\n",
    "        for (start, end) in windows(data, window_size, step):\n",
    "            if(len(data[start:end]) == window_size):\n",
    "                segments = segments + [data[start:end]]\n",
    "        return np.array(segments)\n",
    "\n",
    "    def segment_dataset(X, window_size, step):\n",
    "        win_x = []\n",
    "        for i in range(X.shape[0]):\n",
    "            win_x = win_x + [segment_signal_without_transition(X[i], window_size, step)]\n",
    "        win_x = np.array(win_x)\n",
    "        return win_x\n",
    "\n",
    "    train_raw_x = np.transpose(train_X, [0, 2, 1])\n",
    "    test_raw_x = np.transpose(test_X, [0, 2, 1])\n",
    "\n",
    "\n",
    "    train_win_x = segment_dataset(train_raw_x, window_size, step)\n",
    "    print(\"train_win_x shape: \", train_win_x.shape)\n",
    "    test_win_x = segment_dataset(test_raw_x, window_size, step)\n",
    "    print(\"test_win_x shape: \", test_win_x.shape)\n",
    "\n",
    "    # [trial, window, channel, time_length]\n",
    "    train_win_x = np.transpose(train_win_x, [0, 1, 3, 2])\n",
    "    print(\"train_win_x shape: \", train_win_x.shape)\n",
    "\n",
    "    test_win_x = np.transpose(test_win_x, [0, 1, 3, 2])\n",
    "    print(\"test_win_x shape: \", test_win_x.shape)\n",
    "\n",
    "\n",
    "    # [trial, window, channel, time_length, 1]\n",
    "    train_x = np.expand_dims(train_win_x, axis = 4)\n",
    "    test_x = np.expand_dims(test_win_x, axis = 4)\n",
    "\n",
    "    num_timestep = train_x.shape[1]\n",
    "    ###########################################################################\n",
    "    # set model parameters\n",
    "    ###########################################################################\n",
    "    # kernel parameter\n",
    "    kernel_height_1st\t= 22\n",
    "    kernel_width_1st \t= 45\n",
    "\n",
    "    kernel_stride\t\t= 1\n",
    "\n",
    "    conv_channel_num\t= 40\n",
    "\n",
    "    # pooling parameter\n",
    "    pooling_height_1st \t= 1\n",
    "    pooling_width_1st \t= 56\n",
    "\n",
    "    pooling_stride_1st = 10\n",
    "\n",
    "    # full connected parameter\n",
    "    attention_size = 512\n",
    "    n_hidden_state = 64\n",
    "\n",
    "    ###########################################################################\n",
    "    # set dataset parameters\n",
    "    ###########################################################################\n",
    "    # input channel\n",
    "    input_channel_num = 1\n",
    "\n",
    "    # input height \n",
    "    input_height = train_x.shape[2]\n",
    "\n",
    "    # input width\n",
    "    input_width = train_x.shape[3]\n",
    "\n",
    "    # prediction class\n",
    "    num_labels = 2\n",
    "    ###########################################################################\n",
    "    # set training parameters\n",
    "    ###########################################################################\n",
    "    # set learning rate\n",
    "    learning_rate = 5e-4\n",
    "\n",
    "    # set maximum traing epochs\n",
    "    training_epochs = 120\n",
    "\n",
    "    # set batch size\n",
    "    batch_size = 10\n",
    "\n",
    "    # set dropout probability\n",
    "    dropout_prob = 0.5\n",
    "\n",
    "    # set train batch number per epoch\n",
    "    batch_num_per_epoch = train_x.shape[0]//batch_size\n",
    "\n",
    "    # instance cnn class\n",
    "    padding = 'VALID'\n",
    "\n",
    "    cnn_2d = cnn(padding=padding)\n",
    "\n",
    "    # input placeholder\n",
    "    X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name = 'X')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_labels], name = 'Y')\n",
    "    train_phase = tf.placeholder(tf.bool, name = 'train_phase')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    # first CNN layer\n",
    "    conv_1 = cnn_2d.apply_conv2d(X, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride, train_phase)\n",
    "    print(\"conv 1 shape: \", conv_1.get_shape().as_list())\n",
    "    pool_1 = cnn_2d.apply_max_pooling(conv_1, pooling_height_1st, pooling_width_1st, pooling_stride_1st)\n",
    "    print(\"pool 1 shape: \", pool_1.get_shape().as_list())\n",
    "\n",
    "    pool1_shape = pool_1.get_shape().as_list()\n",
    "    pool1_flat = tf.reshape(pool_1, [-1, pool1_shape[1]*pool1_shape[2]*pool1_shape[3]])\n",
    "\n",
    "    fc_drop = tf.nn.dropout(pool1_flat, keep_prob)\t\n",
    "\n",
    "    lstm_in = tf.reshape(fc_drop, [-1, num_timestep, pool1_shape[1]*pool1_shape[2]*pool1_shape[3]])\n",
    "\n",
    "    ################\n",
    "    # pipline design\n",
    "    #################\n",
    "\n",
    "    ########################## RNN ########################\n",
    "    cells = []\n",
    "    for _ in range(2):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_state, forget_bias=1.0, state_is_tuple=True)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "        cells.append(cell)\n",
    "    lstm_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "    # output ==> [batch, step, n_hidden_state]\n",
    "    rnn_op, states = tf.nn.dynamic_rnn(lstm_cell, lstm_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    ########################## attention ########################\n",
    "    with tf.name_scope('Attention_layer'):\n",
    "        attention_op, alphas = attention(rnn_op, attention_size, time_major = False, return_alphas=True)\n",
    "\n",
    "    attention_drop = tf.nn.dropout(attention_op, keep_prob)\t\n",
    "\n",
    "    ########################## readout ########################\n",
    "    y_ = cnn_2d.apply_readout(attention_drop, rnn_op.shape[2].value, num_labels)\n",
    "\n",
    "    # probability prediction \n",
    "    y_prob = tf.nn.softmax(y_, name = \"y_prob\")\n",
    "\n",
    "    # class prediction \n",
    "    y_pred = tf.argmax(y_prob, 1, name = \"y_pred\")\n",
    "\n",
    "    ########################## loss and optimizer ########################\n",
    "    # cross entropy cost function\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y), name = 'loss')\n",
    "\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "    # set training SGD optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # get correctly predicted object\n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y_), 1), tf.argmax(Y, 1))\n",
    "\n",
    "    ########################## define accuracy ########################\n",
    "    # calculate prediction accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')\n",
    "\n",
    "    #############\n",
    "    # train test \n",
    "    #############\n",
    "    def multiclass_roc_auc_score(y_true, y_score):\n",
    "        assert y_true.shape == y_score.shape\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        n_classes = y_true.shape[1]\n",
    "        # compute ROC curve and ROC area for each class\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        # compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false probtive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        return roc_auc\n",
    "    # run with gpu memory growth\n",
    "    config = tf.ConfigProto()\n",
    "    # config.gpu_options.allow_growth = True\n",
    "\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    best_test_acc = []\n",
    "    train_loss = []\n",
    "    with tf.Session(config=config) as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        best_acc = 0\n",
    "        for epoch in range(training_epochs):\n",
    "            pred_test = np.array([])\n",
    "            true_test = []\n",
    "            prob_test = []\n",
    "            ########################## training process ########################\n",
    "            for b in range(batch_num_per_epoch):\n",
    "                offset = (b * batch_size) % (train_y.shape[0] - batch_size) \n",
    "                batch_x = train_x[offset:(offset + batch_size), :, :, :, :]\n",
    "                batch_x = batch_x.reshape([len(batch_x)*num_timestep, n_channel, window_size, 1])\n",
    "                batch_y = train_y[offset:(offset + batch_size), :]\n",
    "                _, c = session.run([optimizer, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1-dropout_prob, train_phase: True})\n",
    "            # calculate train and test accuracy after each training epoch\n",
    "            if(epoch%1 == 0):\n",
    "                train_accuracy \t= np.zeros(shape=[0], dtype=float)\n",
    "                test_accuracy\t= np.zeros(shape=[0], dtype=float)\n",
    "                train_l \t\t= np.zeros(shape=[0], dtype=float)\n",
    "                test_l\t\t\t= np.zeros(shape=[0], dtype=float)\n",
    "                # calculate train accuracy after each training epoch\n",
    "                for i in range(batch_num_per_epoch):\n",
    "                    ########################## prepare training data ########################\n",
    "                    offset = (i * batch_size) % (train_y.shape[0] - batch_size) \n",
    "                    train_batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "                    train_batch_x = train_batch_x.reshape([len(train_batch_x)*num_timestep, n_channel, window_size, 1])\n",
    "                    train_batch_y = train_y[offset:(offset + batch_size), :]\n",
    "\n",
    "                    ########################## calculate training results ########################\n",
    "                    train_a, train_c = session.run([accuracy, cost], feed_dict={X: train_batch_x, Y: train_batch_y, keep_prob: 1.0, train_phase: False})\n",
    "\n",
    "                    train_l = np.append(train_l, train_c)\n",
    "                    train_accuracy = np.append(train_accuracy, train_a)\n",
    "                print(\"(\"+time.asctime(time.localtime(time.time()))+\") Epoch: \", epoch+1, \" Training Cost: \", np.mean(train_l), \"Training Accuracy: \", np.mean(train_accuracy))\n",
    "                train_acc = train_acc + [np.mean(train_accuracy)]\n",
    "                train_loss = train_loss + [np.mean(train_l)]\n",
    "                # calculate test accuracy after each training epoch\n",
    "                for j in range(batch_num_per_epoch):\n",
    "                    ########################## prepare test data ########################\n",
    "                    offset = (j * batch_size) % (test_y.shape[0] - batch_size) \n",
    "                    test_batch_x = test_x[offset:(offset + batch_size), :, :, :]\n",
    "                    test_batch_x = test_batch_x.reshape([len(test_batch_x)*num_timestep, n_channel, window_size, 1])\n",
    "                    test_batch_y = test_y[offset:(offset + batch_size), :]\n",
    "\n",
    "                    ########################## calculate test results ########################\n",
    "                    test_a, test_c, prob_v, pred_v = session.run([accuracy, cost, y_prob, y_pred], feed_dict={X: test_batch_x, Y: test_batch_y, keep_prob: 1.0, train_phase: False})\n",
    "\n",
    "                    test_accuracy = np.append(test_accuracy, test_a)\n",
    "                    test_l = np.append(test_l, test_c)\n",
    "                    pred_test = np.append(pred_test, pred_v)\n",
    "                    true_test.append(test_batch_y)\n",
    "                    prob_test.append(prob_v)\n",
    "                if np.mean(test_accuracy) > best_acc :\n",
    "                    best_acc = np.mean(test_accuracy)\n",
    "                true_test = np.array(true_test).reshape([-1, num_labels])\n",
    "                prob_test = np.array(prob_test).reshape([-1, num_labels])\n",
    "                auc_roc_test = multiclass_roc_auc_score(y_true=true_test, y_score=prob_test)\n",
    "                f1 = f1_score (y_true=np.argmax(true_test, axis = 1), y_pred=pred_test, average = 'macro')\n",
    "                print(\"(\"+time.asctime(time.localtime(time.time()))+\") Epoch: \", epoch+1, \"Test Cost: \", np.mean(test_l), \n",
    "                                                                                          \"Test Accuracy: \", np.mean(test_accuracy), \n",
    "                                                                                          \"Test f1: \", f1, \n",
    "                                                                                          \"Test AUC: \", auc_roc_test['macro'], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
